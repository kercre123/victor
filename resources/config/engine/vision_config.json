{
  "InitialVisionModes" :
  {
    // If a vision mode is not listed, it will be left in its default state
    "DetectingMarkers"           : true,
    "DetectingFaces"             : false,
    "DetectingPets"              : false,
    "DetectingMotion"            : false,
    "DetectingOverheadEdges"     : false,
    "CheckingQuality"            : true,
    "CheckingWhiteBalance"       : true,
    "EstimatingFacialExpression" : false,
    "DetectingSmileAmount"       : false,
    "DetectingGaze"              : false,
    "DetectingBlinkAmount"       : false,
    "DetectingLaserPoints"       : false,
    "ComputingStatistics"        : false,
    "BuildingOverheadMap"        : false,
    "DetectingVisualObstacles"   : false,
    "RunningNeuralNet"           : false,
    "DetectingIllumination"      : true,
    "NightVision"                : false
  },

  "InitialModeSchedules" :
  {
    // If a vision mode is not listed, it is scheduled to run every frame (if enabled)
    // Specify either an array of bools indicating a specific schedule, or an integer
    // value to process every Nth frame
    "DetectingPets"          : [true, false],
    "DetectingOverheadEdges" : [false, true],
    // CheckingQuality and CheckingAWB run every 8 frames but are interleaved so they don't
    // run at the same time and step on each other's toes
    "CheckingQuality"        : [false, false, false, false, false, false, false, true],
    "CheckingWhiteBalance"   : [false, false, false, true, false, false, false, false],
    "DetectingLaserPoints"   : 2,
    "ComputingStatistics"    : false,  // NOTE: enabled but not scheduled!
    "DetectingGeneralObjects": 1,
    "SavingImages"           : 5,
    "DetectingIllumination"  : 1,
    "NightVision"            : 1
  },

  "ColorImages" : true,        // Whether color images are enabled on startup (can still be toggled later)

  "FaceAlbum" : "robot",       // Name of album on disk to load or "robot" to use the one from the robot

  "FaceDetection" :
  {
    "DetectionMode": "video"   // "video" or "singleImage"
  },

  "FaceRecognition" :
  {
    "RunMode": "asynchronous"  // "synchronous" or "asynchronous"
  },

  "IlluminationDetector" :
  {
    "ClassifierConfigPath"       : "config/engine/vision/illuminationClassifier/illuminationClassifier.json",

    "FeaturePercentileSubsample" : 8,    // Stride for building histogram to compute percentiles
    "IlluminatedMinProbability"  : 0.85, // Min probability to result in 'Illuminated' class
    "DarkenedMaxProbability"     : 0.15, // Max probability to result in 'Darkened' class
    "AllowMovement"              : true  // Continue to run even if robot is moving
  },

  "ImageQuality" :
  {
    // Auto-exposure settings
    "InitialExposureTime_ms"    : 31,    // Sent each time we request camera calibration
    "TargetValue"               : 115,   // [0,255] - Try to make targetPercentile have this value
    "TargetPercentile"          : 0.50,  // [0.0, 1.0]
    "TooDarkValue"              : 20,    // "Too Dark" if HighPercentile is below this
    "TooBrightValue"            : 230,   // "Too Bright" if LowPercentile is above this
    "LowPercentile"             : 0.10,  // [0.0, 1.0]  (Near 0.0)
    "HighPercentile"            : 0.90,  // [0.0, 1.0]  (Near 1.0)
    "MaxChangeFraction"         : 1.0,   // [0.0, 1.0]  (Relative amount we can change current exposure/gain/wb each update)
    "SubSample"                 : 3,     // >= 1
    "MeterFromDetections"       : true,  // Base auto-exposure on detected markers, faces, etc, if any

    "TimeBeforeErrorMessage_ms" :       3000,  // How long VisionSystem must detect "bad" quality before notifying game
    "RepeatedErrorMessageInterval_ms" : 300000 // Time between error messages once triggered
  },

  "NightVision" :
  {
    "MinNumImages"       : 3,   // [1+] Minimum number of images in accumulator before outputting
    "HistSubsample"      : 8,   // [1+] Image subsampling stride for computing histogram for percentile
    "TargetPercentile"   : 50,  // [0-100] Image intensity percentile to compute for contrast adjustment
    "TargetValue"        : 240, // [0-255] Target percentile is scaled to this for contrast adjustment
    "BodyAngleThreshold" : 1,   // Max allowable body angle change in deg
    "BodyPoseThreshold"  : 1,   // Max allowable body movement in mm
    "HeadAngleThreshold" : 1    // Max allowable head angle change in deg
  },

  "PerformanceLogging" :
  {
    "DropStatsWindowLength_sec"         : 30,  // How long to average dropped image stats
    "TimeBetweenProfilerInfoPrints_sec" : 60,  // How often to print Profiler info messages to the logs
    "TimeBetweenProfilerDasLogs_sec"    : 600  // How often to log Profiler DAS events
  },

  "PetTracker" :
  {
    "MaxPets"             : 4,    // Detectable/trackable at the same time
    "MinFaceSize"         : 60,
    "MaxFaceSize"         : 240,
    "DetectionThreshold"  : 900,  // 0 to 1000
    "InitialSearchCycle"  : 4,    // 1 to 45
    "NewSearchCycle"      : 8,    // 5 to 45
    "TrackLostCount"      : 2,    // Number of frames to continue searching for (and reporting) a lost pet
    "TrackSteadiness"     : 15    // 10 to 30 in Percentage change required to actually update size/position
  },

  "MotionDetector" :
  {
  	"HorizontalSize"    : 0.3,    // Fraction of the width of the image to be used for peripheral motion detection (right and left)
  	"VerticalSize"      : 0.3,    // Fraction of the height of the image to be used for peripheral motion detection (top)
  	"IncreaseFactor"    : 500.0,  // The higher this number, the more sensitive is motion detection to motion
  	"DecreaseFactor"    : 0.1,    // The higher this number, the more quickly motion detection forgets about motion
  	"MaxValue"          : 3.0,    // The higher this value, the sooner peripheral motion detection will be triggered
    "CentroidStability" : 0.6     // How quickly should peripheral motion detection track the source of motion.
  },

  "OverheadMap" :
  {
    "NumRows"          : 1000,    // Size of the overhead map. Bigger sizes do not impact computation, only memory
    "NumCols"          : 1000
  },

  "GroundPlaneClassifier" :
  {
    "MaxDepth"            : 7,
    "MinSampleCount"      : 10,
    "TruncatePrunedTree"  : true,
    "Use1SERule"          : true,
    "PositiveWeight"      : 1.0,
    "OnTheFlyTrain"       : false,
    "FileOrDirName"       : "config/engine/vision/groundClassifier/deskClassifier.yaml"
  },

  "NeuralNets" :
  {
    // How frequently to print or log events about timing
    "ProfilingPrintFrequency_ms"     : 10000,
    "ProfilingEventLogFrequency_ms"  : 30000,

    //
    // Model definition:
    //

    // Standalone TF model
    "graphFile"           : "person_localization_victor_6x6_92_accuracy.pb", 
    "memoryMapGraph"      : 0,
    "labelsFile"          : "person-labels.txt", 
    "architecture"        : "custom",
    "inputLayerName"      : "input_1",
    "outputLayerNames"    : "output_node0",
    "outputType"          : "binary_localization",
    "numGridRows"         : 6,
    "numGridCols"         : 6,
    "useFloatInput"       : 1,
    "useGrayscale"        : 0,
    "inputWidth"          : 128, 
    "inputHeight"         : 128, 
    "inputScale"          : 127.5, 
    "inputShift"          : -1, 
    "minScore"            : 0.5,
    "verbose"             : 0,
    "pollPeriod_ms"       : 10


    // // OpenCV DNN w/ MobileNet SSD Model
    // // NOTE: Currently assumes model is symlinked in resources/config/engine/vision/dnn_models to:
    // //       ~/DropBox/DeepLearningVisionModels, shared by andrew (until VIC-1071)
    // "graph"  : "ssd_mobilenet_v1_coco_11_06_2017_frozen.pb",
    // "labels" : "cocostuff-labels.txt",
    // "mode"   : "detection",
    // "input_width" : 224,
    // "input_height": 224,
    // "input_mean_R": 0,
    // "input_mean_G": 0,
    // "input_mean_B": 0,
    // "input_std": 255,
    // "top_K": 1,
    // "min_score": 0.5

    // // SqueezeNet on ImageNet with Caffe
    // // See also: https://github.com/DeepScale/SqueezeNet
    // "graph" : "squeezenet_v1.1",
    // "labels" : "squeezenet_labels.txt",
    // "mode"   : "classification",
    // "input_width" : 227,
    // "input_height" : 227,
    // "input_mean_R" : 104,
    // "input_mean_G" : 117,
    // "input_mean_B" : 123,
    // "input_std"    : 1,
    // "top_K" : 1,
    // "min_score" : 0.5

    // // OpencCV DNN w/ MobileNets classification model
    // //"graph"  : "mobilenet_0.50_128_RockPaperScissors_opencvdnn.pb",
    // "graph"  : "mobilenet_0.75_128_RockPaperScissors_combinedData_opencvdnn.pb",
    // "labels" : "RockPaperScissors_labels.txt",
    // "mode"   : "classification",
    // "input_width"  : 128,
    // "input_height" : 128,
    // "do_crop" : false,
    // "input_mean_R" : 127.5,
    // "input_mean_G" : 127.5,
    // "input_mean_B" : 127.5,
    // "input_std"    : 127.5,
    // "input_layer"  : "input",
    // "output_scores_layer" : "final_result",
    // "top_K" : 1,
    // "min_score" : 0.1

    // "graph"                       : "ssd_mobilenet_v1_coco_11_06_2017_quantized.pb",
    // //config["graph"] = "ssd_inception_v2_coco_11_06_2017_frozen.pb";
    // //config["graph"] = "rfcn_resnet101_coco_11_06_2017_frozen.pb"; // Doesn't work: needs Op "Round"
    // //config["graph"] = "faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017_frozen.pb"; // Doesn't work: needs Op "FloorMod"
    // //config["graph"] = "faster_rcnn_resnet101_coco_11_06_2017_frozen.pb"; // Doesn't work: needs Op "Round"
    // "labels"                      : "cocostuff-labels-no-numbers.txt",
    // "mode"                        : "detection",
    // "input_layer"                 :  "image_tensor",
    // "input_width"                 : 0,
    // "input_height"                : 0,
    // "do_crop"                     : true,
    // "output_scores_layer"         : "detection_scores",
    // "output_classes_layer"        : "detection_classes",
    // "output_boxes_layer"          : "detection_boxes",
    // "output_num_detections_layer" :  "num_detections",
    // "top_K"                       : 5,
    // "min_score"                   : 0.5

    //"graph"         : "inception_v3_2016_08_28_frozen.pb",
    //"labels"        : "imagenet_slim_labels.txt",
    //"input_width"   : 299,
    //"input_height"  : 299,
    //"do_crop"       : true,
    //"input_mean_R"  : 0,
    //"input_mean_G"  : 0,
    //"input_mean_B"  : 0,
    //"input_std"     : 255,
    //"input_layer"   : "input",
    //"output_layer"  : "InceptionV3/Predictions/Reshape_1",
    //"top_K"         : 1  // Only 1 supported for now

  }
}
