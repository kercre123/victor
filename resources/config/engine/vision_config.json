{
  "InitialVisionModes" :
  {
    // If a vision mode is not listed, it will be left in its default state
    "DetectingMarkers"           : true,
    "DetectingFaces"             : false,
    "DetectingPets"              : false,
    "DetectingMotion"            : false,
    "DetectingOverheadEdges"     : false,
    "AutoExposure"               : true,
    "WhiteBalance"               : true,
    "EstimatingFacialExpression" : false,
    "DetectingSmileAmount"       : false,
    "DetectingGaze"              : false,
    "DetectingBlinkAmount"       : false,
    "DetectingLaserPoints"       : false,
    "ComputingStatistics"        : false,
    "BuildingOverheadMap"        : false,
    "DetectingVisualObstacles"   : false,
    "RunningNeuralNet"           : false,
    "DetectingIllumination"      : true
  },

  "InitialModeSchedules" :
  {
    // If a vision mode is not listed, it is scheduled to run every frame (if enabled)
    // Specify either an array of bools indicating a specific schedule, or an integer
    // value to process every Nth frame
    "DetectingPets"          : [true, false],
    "DetectingOverheadEdges" : [false, true],
    "AutoExposure"           : 5,  // NOTE: Should probably be no more frequent than every 5th frame
    "WhiteBalance"           : 5,  // NOTE: Should probably be no more frequent than every 5th frame
    "DetectingLaserPoints"   : 2,
    "ComputingStatistics"    : false,  // NOTE: enabled but not scheduled!
    "RunningNeuralNet"       : 1, // Won't actually run every frame, but as frequently as possible (is asynchronous)
    "SavingImages"           : 1,
    "DetectingIllumination"  : 1
  },

  "ColorImages" : true,        // Whether color images are enabled on startup (can still be toggled later)

  "FaceAlbum" : "default",     
  
  "FaceDetection" :
  {
    "DetectionMode": "video"   // "video" or "singleImage"
  },

  "FaceRecognition" :
  {
    "RunMode": "asynchronous"  // "synchronous" or "asynchronous"
  },

  "IlluminationDetector" :
  {
    "ClassifierConfigPath"       : "config/engine/vision/illuminationClassifier/illuminationClassifier.json",

    "FeaturePercentileSubsample" : 8,    // Stride for building histogram to compute percentiles
    "IlluminatedMinProbability"  : 0.85, // Min probability to result in 'Illuminated' class
    "DarkenedMaxProbability"     : 0.15, // Max probability to result in 'Darkened' class
    "AllowMovement"              : true  // Continue to run even if robot is moving
  },

  "ImageQuality" :
  {
    // Auto-exposure settings
    "InitialExposureTime_ms"    : 31,    // Sent each time we request camera calibration
    "TargetValue"               : 80,    // [0,255] - Try to make targetPercentile have this value
    "TargetPercentile"          : 0.50,  // [0.0, 1.0]
    "CyclingTargetValues"       : [5, 100, 250], // When "cycling" exposure is enabled, the target values to use [0,255]
    "TooDarkValue"              : 20,    // "Too Dark" if HighPercentile is below this
    "TooBrightValue"            : 230,   // "Too Bright" if LowPercentile is above this
    "LowPercentile"             : 0.10,  // [0.0, 1.0]  (Near 0.0)
    "HighPercentile"            : 0.90,  // [0.0, 1.0]  (Near 1.0)
    "MaxChangeFraction"         : 1.0,   // [0.0, 1.0]  (Relative amount we can change current exposure/gain/wb each update)
    "SubSample"                 : 3,     // >= 1
    "MeterFromDetections"       : true,  // Base auto-exposure on detected markers, faces, etc, if any

    "TimeBeforeErrorMessage_ms" :       3000,  // How long VisionSystem must detect "bad" quality before notifying game
    "RepeatedErrorMessageInterval_ms" : 300000 // Time between error messages once triggered
  },

  "NumOpenCvThreads" : 0, // Number of threads to use with OpenCV. 0 means no threading according to docs. Only affects calls from VisionSystem thread.

  "PerformanceLogging" :
  {
    "DropStatsWindowLength_sec"         : 30,  // How long to average dropped image stats
    "TimeBetweenProfilerInfoPrints_sec" : 60,  // How often to print Profiler info messages to the logs
    "TimeBetweenProfilerDasLogs_sec"    : 600  // How often to log Profiler DAS events
  },

  "PetTracker" :
  {
    "MaxPets"             : 4,    // Detectable/trackable at the same time
    "MinFaceSize"         : 60,
    "MaxFaceSize"         : 240,
    "DetectionThreshold"  : 900,  // 0 to 1000
    "InitialSearchCycle"  : 4,    // 1 to 45
    "NewSearchCycle"      : 8,    // 5 to 45
    "TrackLostCount"      : 2,    // Number of frames to continue searching for (and reporting) a lost pet
    "TrackSteadiness"     : 15    // 10 to 30 in Percentage change required to actually update size/position
  },

  "MotionDetector" :
  {
  	"HorizontalSize"    : 0.3,    // Fraction of the width of the image to be used for peripheral motion detection (right and left)
  	"VerticalSize"      : 0.3,    // Fraction of the height of the image to be used for peripheral motion detection (top)
  	"IncreaseFactor"    : 500.0,  // The higher this number, the more sensitive is motion detection to motion
  	"DecreaseFactor"    : 0.1,    // The higher this number, the more quickly motion detection forgets about motion
  	"MaxValue"          : 3.0,    // The higher this value, the sooner peripheral motion detection will be triggered
    "CentroidStability" : 0.6     // How quickly should peripheral motion detection track the source of motion.
  },

  "OverheadMap" :
  {
    "NumRows"          : 1000,    // Size of the overhead map. Bigger sizes do not impact computation, only memory
    "NumCols"          : 1000
  },

  "GroundPlaneClassifier" :
  {
    "MaxDepth"            : 7,
    "MinSampleCount"      : 10,
    "TruncatePrunedTree"  : true,
    "Use1SERule"          : true,
    "PositiveWeight"      : 1.0,
    "OnTheFlyTrain"       : false,
    "FileOrDirName"       : "config/engine/vision/groundClassifier/deskClassifier.yaml"
  },

  "NeuralNets" :
  {
    // How frequently to print or log events about timing
    "ProfilingPrintFrequency_ms"     : 10000,
    "ProfilingEventLogFrequency_ms"  : 30000,

    //
    // Model definition:
    //
    "graphFile"           : "person_localization_victor_6x6_92_accuracy.tflite", 
    "memoryMapGraph"      : 0,
    "labelsFile"          : "person-labels.txt", 
    "architecture"        : "custom",
    "inputLayerName"      : "input_1",
    "outputLayerNames"    : "output_node0",
    "outputType"          : "binary_localization",
    "numGridRows"         : 6,
    "numGridCols"         : 6,
    "useFloatInput"       : 1,
    "useGrayscale"        : 0,
    "inputWidth"          : 128, 
    "inputHeight"         : 128, 
    "inputScale"          : 127.5, // When input is float, data is first divided by scale and then shifted
    "inputShift"          : -1,    // I.e.:  float_input = data / inputScale + inputShift
    "minScore"            : 0.5,
    "verbose"             : 0,
    "pollPeriod_ms"       : 10,
    "timeoutDuration_sec" : 10.0,
    "benchmarkRuns"       : 0


    // // OBJECTNESS MODEL
    // "ProfilingPrintFrequency_ms"     : 10000,
    // "ProfilingEventLogFrequency_ms"  : 30000,
    //
    // //
    // // Model definition:
    // //
    //
    // // Standalone TF model
    // "graphFile"                 : "objectness-mobilenet-v2-height-192-width-192-channels-3-05-2.11.pb", 
    // "memoryMapGraph"            : 0,
    // "labelsFile"                : "person-labels.txt", 
    // "architecture"              : "custom",
    // "inputLayerName"            : "input_1",
    // "outputLayerNames"          : "output_node0",
    // "outputType"                : "segmentation",
    // "numGridRows"               : 6,
    // "numGridCols"               : 6,
    // "useFloatInput"             : 1,
    // "useGrayscale"              : 0,
    // "inputWidth"                : 192, 
    // "inputHeight"               : 192, 
    // "inputScale"                : 127.5, 
    // "inputShift"                : -1, 
    // "minScore"                  : 0.5,
    // "verbose"                   : 1,
    // "pollPeriod_ms"             : 10,
    // "timeoutDuration_sec"       : 100.0,
    // "visualizationDirectory"    : "objectnessResponseMap",
    // "benchmarkRuns"             : 0
  }
}
